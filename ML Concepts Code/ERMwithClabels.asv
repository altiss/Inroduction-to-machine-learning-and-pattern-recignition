% Expected risk minimization with C classes
clear all, close all,

C = 3;
N = 100; % Number of samples
n = 2; % Data dimensionality (must be 2 for plots to work)
gmmParameters.priors = ones(1,C)/C; % uniform priors
gmmParameters.meanVectors = 1.25*n*C*(rand(n,C)); % arbitrary mean vectors
for l = 1:C
    A = eye(n)+0.2*randn(n,n);
    gmmParameters.covMatrices(:,:,l) = A'*A; % arbitrary covariance matrices
end

% Generate data from specified pdf
[x,labels] = generateDataFromGMM(N,gmmParameters); % Generate data
for l = 1:C
    Nclass(l,1) = length(find(labels==l));
end

% Shared computation for both parts
for l = 1:C
    pxgivenl(l,:) = evalGaussianPDF(x,gmmParameters.meanVectors(:,l),gmmParameters.covMatrices(:,:,l)); % Evaluate p(x|L=l)
end
px = gmmParameters.priors*pxgivenl; % Total probability theorem
classPosteriors = pxgivenl.*repmat(gmmParameters.priors',1,N)./repmat(px,C,1); % P(L=l|x)

lossMatrixA = ones(C,C)-eye(C); % For min-Perror design, use 0-1 loss
expectedRisksA = lossMatrixA*classPosteriors; % Expected Risk for each label (rows) for each sample (columns)
[~,decisionsA] = min(expectedRisksA,[],1); % Minimum expected risk decision with 0-1 loss is the same as MAP

if n = 2
    figure(1), clf,
    plot(x(1,label
end

% The matrix computed here is P(D=d|L=l,x)
% It is called the confusion matrix if decisions are class labels.
for d = 1:C % each decision option
    for l = 1:C % each class label
        ConfusionMatrix(d,l) = sum(find(decisionsA==d & labels==l))/sum(find(labels==1));
    end
end
ConfusionMatrix,
    
